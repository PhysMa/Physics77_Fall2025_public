{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e811565",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw12.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae3985",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_seed = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc8b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "#below line allows matplotlib plots to appear in cell output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3705b69e",
   "metadata": {},
   "source": [
    "# **Question 1**: Machine Learning with the Iris Dataset\n",
    "\n",
    "In this question, you'll explore fundamental machine learning techniques using the famous Iris dataset. You'll implement data loading, unsupervised clustering, and supervised classification methods.\n",
    "\n",
    "### Background: The Iris Dataset\n",
    "\n",
    "The Iris dataset is one of the most well-known datasets in machine learning and statistics. It was introduced by Ronald Fisher in 1936 and contains measurements of 150 iris flowers from three different species:\n",
    "- **Iris Setosa**\n",
    "- **Iris Versicolor**\n",
    "- **Iris Virginica**\n",
    "\n",
    "Each sample has four features:\n",
    "1. **Sepal Length** (cm)\n",
    "2. **Sepal Width** (cm)\n",
    "3. **Petal Length** (cm)\n",
    "4. **Petal Width** (cm)\n",
    "\n",
    "The goal is to classify flowers into one of the three species based on these measurements.\n",
    "\n",
    "### Machine Learning Overview\n",
    "\n",
    "We'll explore two types of machine learning:\n",
    "\n",
    "**Unsupervised Learning (K-Means Clustering):**\n",
    "- No labeled data is used during training\n",
    "- Algorithm finds patterns and groups data into clusters\n",
    "- Useful when you don't have labeled examples\n",
    "\n",
    "**Supervised Learning (Classification):**\n",
    "- Labeled data is used to train a model\n",
    "- Model learns to predict labels for new, unseen data\n",
    "- Decision Trees and Neural Networks are popular classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac17497",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## **Part A**: Loading the Iris Dataset\n",
    "\n",
    "In this part, you'll load the Iris dataset using scikit-learn's built-in dataset module.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Write a function `load_iris_data()` that:\n",
    "1. Loads the Iris dataset from sklearn\n",
    "2. Extracts the feature data and class labels\n",
    "3. Returns them as numpy arrays\n",
    "\n",
    "### Background: sklearn.datasets\n",
    "\n",
    "Scikit-learn provides several built-in datasets for practice and testing. The `load_iris()` function returns a dictionary-like object with the following key attributes:\n",
    "- `.data`: The feature matrix (measurements)\n",
    "- `.target`: The class labels (0, 1, or 2 for the three species)\n",
    "- `.feature_names`: Names of the four features\n",
    "- `.target_names`: Names of the three species\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Import and use `sklearn.datasets.load_iris()`\n",
    "- Extract the features (`.data` attribute) as a numpy array\n",
    "- Extract the class labels (`.target` attribute) as a numpy array\n",
    "- Return both arrays as a tuple: `(features, labels)`\n",
    "\n",
    "**Parameters:**\n",
    "- None\n",
    "\n",
    "**Returns:**\n",
    "- `features`: numpy array of shape `(150, 4)`, the feature matrix\n",
    "- `labels`: numpy array of shape `(150,)`, the class labels (0, 1, or 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d52e60a",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def load_iris_data():\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42ca8f0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc751a4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## **Part B**: K-Means Clustering\n",
    "\n",
    "In this part, you'll implement unsupervised clustering using the K-Means algorithm.\n",
    "\n",
    "### Background: K-Means Clustering\n",
    "\n",
    "**K-Means** is an unsupervised learning algorithm that groups data points into `k` clusters based on their features. The algorithm works as follows:\n",
    "\n",
    "1. **Initialize**: Randomly place `k` cluster centers (centroids)\n",
    "2. **Assignment**: Assign each data point to the nearest centroid\n",
    "3. **Update**: Move each centroid to the mean position of all points assigned to it\n",
    "4. **Repeat**: Continue steps 2-3 until convergence\n",
    "\n",
    "Mathematically, K-Means minimizes the within-cluster sum of squares:\n",
    "\n",
    "$$J = \\sum_{i=1}^{k} \\sum_{x \\in C_i} ||x - \\mu_i||^2$$\n",
    "\n",
    "where $C_i$ is cluster $i$ and $\\mu_i$ is its centroid.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Write a function `kmeans_clustering(features, k)` that:\n",
    "1. Takes the feature matrix from Part A\n",
    "2. Applies K-Means clustering with `k` clusters\n",
    "3. Returns the cluster labels assigned to each data point\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Use scikit-learn's K-Means implementation from `sklearn.cluster`\n",
    "- Set `random_state=rng_seed` for reproducible results\n",
    "- Fit the model to the features and extract cluster labels\n",
    "- Return the cluster assignments as a numpy array\n",
    "\n",
    "**Hint:** Look for the `KMeans` class in sklearn.cluster and its `.fit_predict()` method.\n",
    "\n",
    "**Parameters:**\n",
    "- `features`: numpy array of shape `(n_samples, n_features)`, the feature matrix\n",
    "- `k`: int, the number of clusters\n",
    "\n",
    "**Returns:**\n",
    "- `cluster_labels`: numpy array of shape `(n_samples,)`, cluster assignments (0 to k-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf58904",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def kmeans_clustering(features, k):\n",
    "    \n",
    "    return cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d85148f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b27962",
   "metadata": {},
   "source": [
    "### Example: Visualizing K-Means Clustering with Different k Values\n",
    "\n",
    "Let's visualize how K-Means clustering performs with different numbers of clusters. We'll use the first two features (sepal dimensions) for easy visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "features, true_labels = load_iris_data()\n",
    "\n",
    "# Try different values of k\n",
    "k_values = [2, 3, 4, 5]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, k in enumerate(k_values):\n",
    "    # Perform K-means clustering\n",
    "    cluster_labels = kmeans_clustering(features, k=k)\n",
    "    \n",
    "    # Plot using first two features (sepal length and sepal width)\n",
    "    scatter = axes[idx].scatter(features[:, 0], features[:, 1], \n",
    "                                c=cluster_labels, cmap='viridis', \n",
    "                                s=50, alpha=0.7, edgecolors='black')\n",
    "    \n",
    "    axes[idx].set_xlabel('Sepal Length (cm)', fontsize=11)\n",
    "    axes[idx].set_ylabel('Sepal Width (cm)', fontsize=11)\n",
    "    axes[idx].set_title(f'K-Means with k={k} clusters', fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(scatter, ax=axes[idx], label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: The true Iris dataset has 3 species, but K-Means can be applied with any k value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c37bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare K-means clusters with true labels\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# K-means with k=3\n",
    "clusters_3 = kmeans_clustering(features, k=3)\n",
    "scatter1 = axes[0].scatter(features[:, 0], features[:, 1], \n",
    "                           c=clusters_3, cmap='viridis', \n",
    "                           s=50, alpha=0.7, edgecolors='black')\n",
    "axes[0].set_xlabel('Sepal Length (cm)', fontsize=11)\n",
    "axes[0].set_ylabel('Sepal Width (cm)', fontsize=11)\n",
    "axes[0].set_title('K-Means Clustering (k=3)', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "\n",
    "# True labels\n",
    "scatter2 = axes[1].scatter(features[:, 0], features[:, 1], \n",
    "                           c=true_labels, cmap='viridis', \n",
    "                           s=50, alpha=0.7, edgecolors='black')\n",
    "axes[1].set_xlabel('Sepal Length (cm)', fontsize=11)\n",
    "axes[1].set_ylabel('Sepal Width (cm)', fontsize=11)\n",
    "axes[1].set_title('True Species Labels', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Species')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"K-Means often finds similar (but not identical) groupings to the true species labels!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0beff2e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## **Part C**: Decision Tree Classification\n",
    "\n",
    "In this part, you'll train a supervised learning model using Decision Trees and evaluate its performance.\n",
    "\n",
    "### Background: Decision Tree Classifier\n",
    "\n",
    "**Decision Trees** are supervised learning models that make predictions by learning simple decision rules from the training data. The tree structure consists of:\n",
    "\n",
    "- **Root Node**: The entire dataset\n",
    "- **Internal Nodes**: Decision points based on feature values\n",
    "- **Branches**: Outcomes of decisions\n",
    "- **Leaf Nodes**: Final predictions (class labels)\n",
    "\n",
    "For example, a decision might be: \"If petal length < 2.5 cm, classify as Setosa, otherwise continue...\"\n",
    "\n",
    "Decision trees work by recursively splitting the data to maximize **information gain** or minimize **impurity** (measured by Gini impurity or entropy).\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "A **confusion matrix** is a table that shows the performance of a classification model:\n",
    "\n",
    "|                | Predicted Class 0 | Predicted Class 1 | Predicted Class 2 |\n",
    "|----------------|------------------|------------------|------------------|\n",
    "| **True Class 0** | True Positives   | False Negatives  | False Negatives  |\n",
    "| **True Class 1** | False Negatives  | True Positives   | False Negatives  |\n",
    "| **True Class 2** | False Negatives  | False Negatives  | True Positives   |\n",
    "\n",
    "Diagonal elements represent correct predictions, while off-diagonal elements represent misclassifications.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Write a function `train_decision_tree(features, labels, show_plot=False)` that:\n",
    "1. Trains a Decision Tree classifier on the full dataset\n",
    "2. Makes predictions on the same data\n",
    "3. Computes and optionally plots the confusion matrix\n",
    "4. Returns the trained model and confusion matrix\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Use `sklearn.tree.DecisionTreeClassifier` with `random_state=rng_seed`\n",
    "- Fit the model using `.fit(features, labels)`\n",
    "- Make predictions using `.predict(features)`\n",
    "- Compute the confusion matrix using `sklearn.metrics.confusion_matrix`\n",
    "- If `show_plot=True`, create a heatmap of the confusion matrix using matplotlib\n",
    "- Return both the trained model and the confusion matrix as numpy array\n",
    "\n",
    "**Hint:** For plotting, use `plt.imshow()` with a colormap and add labels for clarity.\n",
    "\n",
    "**Parameters:**\n",
    "- `features`: numpy array of shape `(n_samples, n_features)`, the feature matrix\n",
    "- `labels`: numpy array of shape `(n_samples,)`, the true class labels\n",
    "- `show_plot`: bool, whether to display the confusion matrix plot (default: False)\n",
    "\n",
    "**Returns:**\n",
    "- `model`: trained DecisionTreeClassifier object\n",
    "- `conf_matrix`: numpy array, the confusion matrix\n",
    "- `fig`: matplotlib figure object (or None if show_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb78a68d",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def train_decision_tree(features, labels, show_plot=False):\n",
    "    \n",
    "    return model, conf_matrix, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce78ea4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625949b2",
   "metadata": {},
   "source": [
    "### Example: Visualizing Decision Tree Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e243151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and visualize\n",
    "features, labels = load_iris_data()\n",
    "model, conf_matrix, fig = train_decision_tree(features, labels, show_plot=True)\n",
    "\n",
    "# Calculate and display accuracy\n",
    "accuracy = np.trace(conf_matrix) / np.sum(conf_matrix)\n",
    "print(f\"Decision Tree Accuracy: {accuracy:.1%}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nNote: Decision trees can achieve perfect accuracy on training data,\")\n",
    "print(\"but may overfit. In practice, we should use train/test splits!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266175ff",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## **Part D**: Neural Network Classification (MLPClassifier)\n",
    "\n",
    "In this part, you'll train a neural network classifier using scikit-learn's Multi-Layer Perceptron (MLP).\n",
    "\n",
    "### Background: Neural Networks\n",
    "\n",
    "**Artificial Neural Networks** are inspired by biological neurons and consist of layers of interconnected nodes:\n",
    "\n",
    "- **Input Layer**: Receives the feature values\n",
    "- **Hidden Layer(s)**: Intermediate layers that learn complex patterns\n",
    "- **Output Layer**: Produces class probabilities\n",
    "\n",
    "Each connection has a **weight** that is adjusted during training. The network learns by:\n",
    "1. Making predictions (forward propagation)\n",
    "2. Computing the error\n",
    "3. Adjusting weights to reduce error (backpropagation)\n",
    "\n",
    "The **Multi-Layer Perceptron (MLP)** is a feedforward neural network that uses:\n",
    "- **Activation functions**: Non-linear transformations (e.g., ReLU, tanh)\n",
    "- **Optimization**: Gradient descent to minimize loss\n",
    "- **Regularization**: Techniques to prevent overfitting\n",
    "\n",
    "### Mathematical Background\n",
    "\n",
    "For a simple neural network with one hidden layer:\n",
    "\n",
    "$$h = \\text{activation}(W_1 \\cdot x + b_1)$$\n",
    "$$y = \\text{softmax}(W_2 \\cdot h + b_2)$$\n",
    "\n",
    "where:\n",
    "- $x$ is the input features\n",
    "- $W_1, b_1$ are weights and biases for the hidden layer\n",
    "- $h$ is the hidden layer output\n",
    "- $W_2, b_2$ are weights and biases for the output layer\n",
    "- $y$ is the predicted class probabilities\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Write a function `train_neural_network(features, labels, show_plot=False)` that:\n",
    "1. Trains an MLP classifier on the full Iris dataset\n",
    "2. Makes predictions and computes the confusion matrix\n",
    "3. Optionally plots the confusion matrix\n",
    "4. Returns the trained model, confusion matrix, and figure\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Use `sklearn.neural_network.MLPClassifier`\n",
    "- Set `random_state=rng_seed` and `max_iter=1000` (or more if needed)\n",
    "- Choose appropriate hyperparameters (hidden layer sizes, activation function, solver, etc.)\n",
    "- The model should achieve at least 90% accuracy on the training data\n",
    "- Follow the same return pattern as Part C: `(model, conf_matrix, fig)`\n",
    "\n",
    "**Parameters:**\n",
    "- `features`: numpy array of shape `(n_samples, n_features)`, the feature matrix\n",
    "- `labels`: numpy array of shape `(n_samples,)`, the true class labels\n",
    "- `show_plot`: bool, whether to display the confusion matrix plot (default: False)\n",
    "\n",
    "**Returns:**\n",
    "- `model`: trained MLPClassifier object\n",
    "- `conf_matrix`: numpy array, the confusion matrix\n",
    "- `fig`: matplotlib figure object (or None if show_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755423f",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def train_neural_network(features, labels, show_plot=False):\n",
    "    \n",
    "    return model, conf_matrix, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aaaca1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdbe22b",
   "metadata": {},
   "source": [
    "### Example: Comparing All Three Methods\n",
    "\n",
    "Let's compare the performance of K-Means, Decision Tree, and Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48620786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "features, labels = load_iris_data()\n",
    "\n",
    "# Train all three models\n",
    "print(\"Training models...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# K-Means (unsupervised)\n",
    "clusters = kmeans_clustering(features, k=3)\n",
    "print(\"✓ K-Means clustering complete\")\n",
    "\n",
    "# Decision Tree (supervised)\n",
    "dt_model, dt_conf, dt_fig = train_decision_tree(features, labels)\n",
    "dt_accuracy = np.trace(dt_conf) / np.sum(dt_conf)\n",
    "print(f\"✓ Decision Tree trained - Accuracy: {dt_accuracy:.1%}\")\n",
    "\n",
    "# Neural Network (supervised)\n",
    "nn_model, nn_conf, nn_fig = train_neural_network(features, labels)\n",
    "nn_accuracy = np.trace(nn_conf) / np.sum(nn_conf)\n",
    "print(f\"✓ Neural Network trained - Accuracy: {nn_accuracy:.1%}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"\\nNote: K-Means is unsupervised (no accuracy metric),\")\n",
    "print(\"while Decision Tree and Neural Network use labeled data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe3fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrices side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Decision Tree confusion matrix\n",
    "im1 = axes[0].imshow(dt_conf, cmap='Blues', interpolation='nearest')\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=11)\n",
    "axes[0].set_ylabel('True Label', fontsize=11)\n",
    "axes[0].set_title(f'Decision Tree\\nAccuracy: {dt_accuracy:.1%}', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xticks(np.arange(3))\n",
    "axes[0].set_yticks(np.arange(3))\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Add text annotations for decision tree\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        text = axes[0].text(j, i, dt_conf[i, j],\n",
    "                           ha=\"center\", va=\"center\", \n",
    "                           color=\"black\" if dt_conf[i, j] < dt_conf.max()/2 else \"white\",\n",
    "                           fontsize=14, fontweight='bold')\n",
    "\n",
    "# Neural Network confusion matrix\n",
    "im2 = axes[1].imshow(nn_conf, cmap='Greens', interpolation='nearest')\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=11)\n",
    "axes[1].set_ylabel('True Label', fontsize=11)\n",
    "axes[1].set_title(f'Neural Network\\nAccuracy: {nn_accuracy:.1%}', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xticks(np.arange(3))\n",
    "axes[1].set_yticks(np.arange(3))\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# Add text annotations for neural network\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        text = axes[1].text(j, i, nn_conf[i, j],\n",
    "                           ha=\"center\", va=\"center\", \n",
    "                           color=\"black\" if nn_conf[i, j] < nn_conf.max()/2 else \"white\",\n",
    "                           fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBoth supervised methods achieve excellent performance on the Iris dataset!\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a6eea91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03018fa0",
   "metadata": {},
   "source": [
    "# **Question 2**: Regression with the Diabetes Dataset\n",
    "\n",
    "In this question, you'll explore **regression** - predicting continuous values rather than discrete classes. You'll work with the diabetes dataset to predict disease progression and learn about proper train/test splitting and data standardization.\n",
    "\n",
    "### Background: Regression vs Classification\n",
    "\n",
    "While classification predicts discrete categories (like iris species), **regression** predicts continuous numerical values. Examples include:\n",
    "- Predicting house prices from features\n",
    "- Forecasting temperature from weather data\n",
    "- Estimating disease progression from medical measurements\n",
    "\n",
    "### The Diabetes Dataset\n",
    "\n",
    "The diabetes dataset contains 442 samples with 10 baseline features:\n",
    "- Age\n",
    "- Sex\n",
    "- Body mass index (BMI)\n",
    "- Average blood pressure\n",
    "- Six blood serum measurements\n",
    "\n",
    "The target variable is a quantitative measure of disease progression one year after baseline.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**Train/Test Split:**\n",
    "- Training data: Used to fit the model\n",
    "- Test/validation data: Used to evaluate performance on unseen data\n",
    "- Prevents overfitting and provides realistic performance estimates\n",
    "\n",
    "**Data Standardization:**\n",
    "- Neural networks perform better when features are on similar scales\n",
    "- Standardization: Transform features to have mean=0 and std=1\n",
    "- Formula: $z = \\frac{x - \\mu}{\\sigma}$\n",
    "- **Important:** Fit standardization on training data only, then apply to test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccda1f2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## **Part A**: Loading and Preprocessing the Diabetes Dataset\n",
    "\n",
    "In this part, you'll load the diabetes dataset, split it into training and test sets, and standardize the features.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Write a function `prepare_diabetes_data(train_fraction=0.8)` that:\n",
    "1. Loads the diabetes dataset from sklearn\n",
    "2. Splits the data into training and test sets\n",
    "3. Standardizes both feature sets separately (important!)\n",
    "4. Returns the preprocessed training and test data\n",
    "\n",
    "### Background: Why Standardize?\n",
    "\n",
    "Neural networks are sensitive to feature scales. Features with larger values can dominate the learning process. **Standardization** transforms each feature to have:\n",
    "- Mean ($\\mu$) = 0\n",
    "- Standard deviation ($\\sigma$) = 1\n",
    "\n",
    "The transformation is: $z = \\frac{x - \\mu}{\\sigma}$\n",
    "\n",
    "**Critical:** Always fit the standardization parameters (mean, std) on the training data only, then apply the same transformation to the test data. This prevents \"data leakage\" where test information influences training.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Use `sklearn.datasets.load_diabetes()` to load the dataset\n",
    "- Use `sklearn.model_selection.train_test_split()` with `random_state=rng_seed`\n",
    "- Use `sklearn.preprocessing.StandardScaler` for standardization\n",
    "  - Fit the scaler on training features only: `.fit(X_train)`\n",
    "  - Transform training features: `.transform(X_train)`\n",
    "  - Transform test features using the same scaler: `.transform(X_test)`\n",
    "- Return four arrays: `(X_train, X_test, y_train, y_test)`\n",
    "\n",
    "**Hint:** The StandardScaler class has methods `.fit()`, `.transform()`, and `.fit_transform()`.\n",
    "\n",
    "**Parameters:**\n",
    "- `train_fraction`: float, fraction of data to use for training (default: 0.8)\n",
    "\n",
    "**Returns:**\n",
    "- `X_train`: numpy array, standardized training features\n",
    "- `X_test`: numpy array, standardized test features  \n",
    "- `y_train`: numpy array, training targets\n",
    "- `y_test`: numpy array, test targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e497ddf2",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def prepare_diabetes_data(train_fraction=0.8):\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cc119a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c255859",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## **Part B**: Training a Neural Network Regressor\n",
    "\n",
    "In this part, you'll train a Multi-Layer Perceptron regressor and visualize its training progress.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Write a function `train_mlp_regressor(X_train, X_test, y_train, y_test, show_plot=False)` that:\n",
    "1. Trains an MLP regressor on the training data\n",
    "2. Tracks validation scores during training (note: validation is taken from training data)\n",
    "3. Computes the test R² score on the actual test set\n",
    "4. Optionally plots training and validation scores over epochs\n",
    "5. Returns the trained model, test R² score, and figure\n",
    "\n",
    "### Background: MLPRegressor and Validation Curves\n",
    "\n",
    "**MLPRegressor** is scikit-learn's neural network for regression. Unlike MLPClassifier, it predicts continuous values.\n",
    "\n",
    "**Validation during training:**\n",
    "- Training score: Performance on training data (can overfit)\n",
    "- Validation score: Performance on a subset held out from training data\n",
    "- Test score: Performance on completely separate test data (most realistic)\n",
    "- Monitoring validation helps detect overfitting during training\n",
    "\n",
    "The MLPRegressor can track validation scores by setting:\n",
    "- `validation_fraction`: Portion of training data to use for validation\n",
    "- `early_stopping=True`: Stop training if validation score doesn't improve\n",
    "- The model stores loss curves in `.loss_curve_` and `.validation_scores_` attributes\n",
    "\n",
    "**Important:** Since validation is taken from the training data, we also compute the test R² score on the true test set to get an unbiased performance estimate.\n",
    "\n",
    "### Evaluation Metric: R² Score\n",
    "\n",
    "For regression, we use the **coefficient of determination** ($R^2$):\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (y_i - \\bar{y})^2}$$\n",
    "\n",
    "where:\n",
    "- $y_i$ are true values\n",
    "- $\\hat{y}_i$ are predicted values\n",
    "- $\\bar{y}$ is the mean of true values\n",
    "\n",
    "$R^2 = 1$ is perfect prediction, $R^2 = 0$ means predicting the mean.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Use `sklearn.neural_network.MLPRegressor`\n",
    "- Set `random_state=rng_seed`, `max_iter=500` (or more if needed)\n",
    "- Enable validation tracking with `validation_fraction=0.1` and `early_stopping=True`\n",
    "- Choose appropriate hyperparameters (hidden layers, activation, etc.)\n",
    "- If `show_plot=True`, create a plot showing:\n",
    "  - Training loss over iterations (from `.loss_curve_`)\n",
    "  - Validation score over iterations (from `.validation_scores_`)\n",
    "  - Use two y-axes if needed (loss decreases, score increases)\n",
    "- Compute the test R² score using `.score(X_test, y_test)`\n",
    "- Return the trained model, test R² score, and figure\n",
    "\n",
    "**Hint:** The `.loss_curve_` attribute contains training loss per epoch, and `.validation_scores_` contains validation R² scores. Use `model.score(X_test, y_test)` to get the test R² score.\n",
    "\n",
    "**Parameters:**\n",
    "- `X_train`: numpy array, standardized training features\n",
    "- `X_test`: numpy array, standardized test features\n",
    "- `y_train`: numpy array, training targets\n",
    "- `y_test`: numpy array, test targets\n",
    "- `show_plot`: bool, whether to display the training curves (default: False)\n",
    "\n",
    "**Returns:**\n",
    "- `model`: trained MLPRegressor object\n",
    "- `test_r2`: float, R² score on the test set\n",
    "- `fig`: matplotlib figure object (or None if show_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c62d5e",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def train_mlp_regressor(X_train, X_test, y_train, y_test, show_plot=False):\n",
    "    \n",
    "    return model, test_r2, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e401858",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c8fe42",
   "metadata": {},
   "source": [
    "### Example: Complete Regression Pipeline\n",
    "\n",
    "Let's demonstrate the full pipeline from data preparation to model training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f466086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "print(\"Step 1: Loading and preprocessing data...\")\n",
    "X_train, X_test, y_train, y_test = prepare_diabetes_data(train_fraction=0.8)\n",
    "\n",
    "print(f\"  Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"  Number of features: {X_train.shape[1]}\")\n",
    "print(f\"  Training features - Mean: {np.mean(X_train):.2e}, Std: {np.mean(np.std(X_train, axis=0)):.2f}\")\n",
    "print()\n",
    "\n",
    "# Train the model\n",
    "print(\"Step 2: Training MLP Regressor...\")\n",
    "model, test_score, fig = train_mlp_regressor(X_train, X_test, y_train, y_test, show_plot=True)\n",
    "print(f\"  Training completed in {model.n_iter_} iterations\")\n",
    "print()\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Step 3: Evaluating model performance...\")\n",
    "train_score = model.score(X_train, y_train)\n",
    "\n",
    "print(f\"  Training R² score: {train_score:.4f}\")\n",
    "print(f\"  Test R² score: {test_score:.4f}\")\n",
    "\n",
    "if train_score - test_score > 0.1:\n",
    "    print(f\"  ⚠ Warning: Training score is much higher than test score\")\n",
    "    print(f\"    This suggests some overfitting.\")\n",
    "else:\n",
    "    print(f\"  ✓ Good generalization - train and test scores are similar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d9cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual values\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training data\n",
    "axes[0].scatter(y_train, y_train_pred, alpha=0.5, s=30, edgecolors='black', linewidth=0.5)\n",
    "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \n",
    "             'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[0].set_xlabel('Actual Disease Progression', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Disease Progression', fontsize=12)\n",
    "axes[0].set_title(f'Training Set\\nR² = {train_score:.4f}', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test data\n",
    "axes[1].scatter(y_test, y_test_pred, alpha=0.5, s=30, edgecolors='black', linewidth=0.5, color='green')\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[1].set_xlabel('Actual Disease Progression', fontsize=12)\n",
    "axes[1].set_ylabel('Predicted Disease Progression', fontsize=12)\n",
    "axes[1].set_title(f'Test Set\\nR² = {test_score:.4f}', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPoints closer to the red line indicate better predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab6262c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Required disclosure of use of AI technology\n",
    "\n",
    "Please indicate whether you used AI to complete this homework. If you did, explain how you used it in the python cell below, as a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e28e65b",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# write ai disclosure here:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab230f0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit.\n",
    "\n",
    "Upload the .zip file to Gradescope!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8578b048",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.export(pdf=False, force_save=True, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dd4cf1",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otter_grading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result = load_iris_data()\n>>> test_result = isinstance(result, tuple) and len(result) == 2\n>>> bool(test_result)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> features, labels = load_iris_data()\n>>> test_result = labels.shape == (150,)\n>>> bool(test_result)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> features, labels = load_iris_data()\n>>> clusters = kmeans_clustering(features, k=3)\n>>> test_result = isinstance(clusters, np.ndarray)\n>>> bool(test_result)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> features, labels = load_iris_data()\n>>> clusters = kmeans_clustering(features, k=3)\n>>> test_result = clusters.shape == (150,)\n>>> bool(test_result)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> features, labels = load_iris_data()\n>>> result = train_decision_tree(features, labels)\n>>> test_result = isinstance(result, tuple) and len(result) == 3\n>>> bool(test_result)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> features, labels = load_iris_data()\n>>> model, conf_matrix, fig = train_decision_tree(features, labels)\n>>> accuracy = np.trace(conf_matrix) / np.sum(conf_matrix)\n>>> test_result = accuracy > 0.9\n>>> bool(test_result)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1d": {
     "name": "q1d",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> features, labels = load_iris_data()\n>>> result = train_neural_network(features, labels)\n>>> test_result = isinstance(result, tuple) and len(result) == 3\n>>> bool(test_result)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> features, labels = load_iris_data()\n>>> model, conf_matrix, fig = train_neural_network(features, labels)\n>>> accuracy = np.trace(conf_matrix) / np.sum(conf_matrix)\n>>> test_result = accuracy > 0.9\n>>> bool(test_result)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result = prepare_diabetes_data()\n>>> test_result = isinstance(result, tuple) and len(result) == 4\n>>> bool(test_result)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> X_train, X_test, y_train, y_test = prepare_diabetes_data(train_fraction=0.8)\n>>> total_samples = len(y_train) + len(y_test)\n>>> actual_train_fraction = len(y_train) / total_samples\n>>> test_result = 0.75 < actual_train_fraction < 0.85\n>>> bool(test_result)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X_train, X_test, y_train, y_test = prepare_diabetes_data()\n>>> result = train_mlp_regressor(X_train, X_test, y_train, y_test)\n>>> test_result = isinstance(result, tuple) and len(result) == 3\n>>> bool(test_result)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> from sklearn.neural_network import MLPRegressor\n>>> X_train, X_test, y_train, y_test = prepare_diabetes_data()\n>>> model, test_r2, fig = train_mlp_regressor(X_train, X_test, y_train, y_test)\n>>> test_result = isinstance(model, MLPRegressor)\n>>> bool(test_result)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> X_train, X_test, y_train, y_test = prepare_diabetes_data()\n>>> model, test_r2, fig = train_mlp_regressor(X_train, X_test, y_train, y_test)\n>>> test_result = test_r2 > 0.3\n>>> bool(test_result)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
