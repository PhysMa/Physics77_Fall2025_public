{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8c1a5d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"ws12.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4e56d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_seed = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23502e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "#below line allows matplotlib plots to appear in cell output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8eb63e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# **Question 1**: Binary Classification on the Two Moons Dataset\n",
    "\n",
    "In this question, you'll explore **binary classification** using machine learning methods on a classic synthetic dataset: the **two moons dataset**. This dataset consists of two interleaving half-moon shapes, making it a challenging problem for linear classifiers but ideal for demonstrating the power of non-linear methods.\n",
    "\n",
    "## Background: Binary Classification\n",
    "\n",
    "**Binary classification** is a supervised learning task where we want to predict which of two classes a data point belongs to. Given:\n",
    "- **Features**: $\\mathbf{x} = (x_1, x_2, ..., x_n)$ - the input variables describing each data point\n",
    "- **Labels**: $y \\in \\{0, 1\\}$ - the class each data point belongs to\n",
    "\n",
    "We want to learn a function $f(\\mathbf{x})$ that predicts the correct label for new, unseen data points.\n",
    "\n",
    "## The Two Moons Dataset\n",
    "\n",
    "The two moons dataset is a 2D dataset where:\n",
    "- Each point has 2 features: $(x_1, x_2)$ representing coordinates in the plane\n",
    "- Points belong to one of two classes (0 or 1), forming two interleaving half-moon shapes\n",
    "- The dataset can include noise, making the classification boundary less clear\n",
    "\n",
    "This dataset is particularly useful for testing classification algorithms because:\n",
    "1. It's **not linearly separable** - you can't draw a straight line to perfectly separate the classes\n",
    "2. It's **visually interpretable** - we can plot the data and decision boundaries in 2D\n",
    "3. It tests an algorithm's ability to learn **non-linear decision boundaries**\n",
    "\n",
    "## **Part A**: Generate Two Moons Dataset\n",
    "\n",
    "Implement `generate_two_moons(n_samples, noise, show_plot=False)` that generates the two moons dataset using scikit-learn.\n",
    "\n",
    "**Requirements:**\n",
    "- Use `sklearn.datasets.make_moons(n_samples=n_samples, noise=noise, random_state=rng_seed)`\n",
    "- Return features `X` as a numpy array of shape `(n_samples, 2)` and labels `y` as a numpy array of shape `(n_samples,)`\n",
    "- If `show_plot=True`, create a scatter plot showing the two classes with different colors\n",
    "- Plot configuration (if `show_plot=True`):\n",
    "  - Figure size: (10, 8)\n",
    "  - Use `plt.scatter()` to plot each class separately\n",
    "  - Class 0: color='blue', label='Class 0', s=50, alpha=0.6\n",
    "  - Class 1: color='red', label='Class 1', s=50, alpha=0.6\n",
    "  - X-axis label: \"Feature 1\"\n",
    "  - Y-axis label: \"Feature 2\"\n",
    "  - Title: \"Two Moons Dataset\"\n",
    "  - Grid with alpha=0.3\n",
    "  - Legend\n",
    "  - Equal aspect ratio using `ax.set_aspect('equal')`\n",
    "- Only call `plt.show()` if `show_plot=True`\n",
    "- Return the figure object (or None if `show_plot=False`)\n",
    "\n",
    "**Parameters:**\n",
    "- `n_samples`: int, total number of data points to generate\n",
    "- `noise`: float, standard deviation of Gaussian noise added to the data\n",
    "- `show_plot`: bool, default False. If True, display the plot\n",
    "\n",
    "**Returns:**\n",
    "- `X`: numpy array of shape (n_samples, 2), the feature matrix\n",
    "- `y`: numpy array of shape (n_samples,), the label vector\n",
    "- `fig`: matplotlib figure object (or None if show_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0cf6ce",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def generate_two_moons(n_samples, noise, show_plot=False):\n",
    "    from sklearn.datasets import make_moons\n",
    "    \n",
    "    # Generate the two moons dataset using make_moons\n",
    "    # Use random_state=rng_seed for reproducibility\n",
    "    \n",
    "    \n",
    "    #   - Plot Class 0 points in blue\n",
    "    #   - Plot Class 1 points in red\n",
    "    #   - Set appropriate labels, title, grid, legend\n",
    "    #   - Set equal aspect ratio\n",
    "    \n",
    "    if show_plot:\n",
    "        # Show plot if requested\n",
    "        plt.show()\n",
    "    \n",
    "    return X, y, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca65361c",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Example: Generate and visualize the two moons dataset with different noise levels\n",
    "\n",
    "# Example 1: Low noise\n",
    "print(\"Example 1: Two moons with low noise (0.05)\")\n",
    "X1, y1, fig1 = generate_two_moons(n_samples=200, noise=0.05, show_plot=True)\n",
    "print(f\"Data shape: X={X1.shape}, y={y1.shape}\")\n",
    "print()\n",
    "\n",
    "# Example 2: Medium noise\n",
    "print(\"Example 2: Two moons with medium noise (0.15)\")\n",
    "X2, y2, fig2 = generate_two_moons(n_samples=200, noise=0.15, show_plot=True)\n",
    "print(f\"Data shape: X={X2.shape}, y={y2.shape}\")\n",
    "print()\n",
    "\n",
    "# Example 3: High noise\n",
    "print(\"Example 3: Two moons with high noise (0.25)\")\n",
    "X3, y3, fig3 = generate_two_moons(n_samples=200, noise=0.25, show_plot=True)\n",
    "print(f\"Data shape: X={X3.shape}, y={y3.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51d680",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34e994d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## **Part B**: Linear Support Vector Machine Classification\n",
    "\n",
    "Implement `classify_with_svm(X, y, show_plot=False)` that trains a **Linear Support Vector Machine (SVM)** classifier and visualizes the decision boundary.\n",
    "\n",
    "### Background: Support Vector Machines\n",
    "\n",
    "A **Support Vector Machine (SVM)** is a powerful classification algorithm that finds the optimal hyperplane (a line in 2D, a plane in 3D, etc.) that separates the classes with the maximum margin.\n",
    "\n",
    "**Linear SVM** finds a linear decision boundary:\n",
    "$$f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{w}$ is the weight vector (perpendicular to the decision boundary)\n",
    "- $b$ is the bias term\n",
    "- Points are classified based on the sign of $f(\\mathbf{x})$\n",
    "\n",
    "**Key Limitation**: Linear SVMs can only learn linear decision boundaries. This works well for linearly separable data but struggles with datasets like the two moons that require non-linear boundaries.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Train the classifier**:\n",
    "   - Use `sklearn.svm.LinearSVC(random_state=rng_seed, max_iter=10000)`\n",
    "   - Fit the model using `classifier.fit(X, y)`\n",
    "\n",
    "2. **Create visualization meshgrid**:\n",
    "   - Determine the range: `x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5`\n",
    "   - Determine the range: `y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5`\n",
    "   - Create meshgrid with step size 0.02: `xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))`\n",
    "   - Flatten and stack: `mesh_points = np.c_[xx.ravel(), yy.ravel()]`\n",
    "   - Predict on mesh: `Z = classifier.predict(mesh_points)`\n",
    "   - Reshape predictions: `Z = Z.reshape(xx.shape)`\n",
    "\n",
    "3. **Plot configuration** (if `show_plot=True`):\n",
    "   - Figure size: (10, 8)\n",
    "   - Use `ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu_r')` to show decision regions\n",
    "   - Scatter plot training data with Class 0 in blue and Class 1 in red (same as Part A)\n",
    "   - X-axis label: \"Feature 1\"\n",
    "   - Y-axis label: \"Feature 2\"\n",
    "   - Title: \"Linear SVM Decision Boundary\"\n",
    "   - Grid with alpha=0.3\n",
    "   - Legend\n",
    "   - Equal aspect ratio using `ax.set_aspect('equal')`\n",
    "   - Only call `plt.show()` if `show_plot=True`\n",
    "\n",
    "**Parameters:**\n",
    "- `X`: numpy array of shape (n_samples, 2), feature matrix from Part A\n",
    "- `y`: numpy array of shape (n_samples,), label vector from Part A\n",
    "- `show_plot`: bool, default False. If True, display the plot\n",
    "\n",
    "**Returns:**\n",
    "- `classifier`: trained LinearSVC object\n",
    "- `fig`: matplotlib figure object (or None if show_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41f7c5",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def classify_with_svm(X, y, show_plot=False):\n",
    "    from sklearn.svm import LinearSVC\n",
    "    \n",
    "    # Train the Linear SVM classifier\n",
    "    # Use: LinearSVC(random_state=rng_seed, max_iter=10000)\n",
    "    # Fit using: classifier.fit(X, y)\n",
    "    \n",
    "    # Create plot if show_plot=True:\n",
    "    #   1. Create meshgrid covering data range (with 0.5 padding)\n",
    "    #      Use step size 0.02 for smooth visualization\n",
    "    #   2. Predict class for all meshgrid points\n",
    "    #   3. Use contourf to show decision regions with alpha=0.3, cmap='RdBu_r'\n",
    "    #   4. Scatter plot training data (same colors as Part A)\n",
    "    #   5. Set labels, title, grid, legend, and equal aspect ratio\n",
    "    \n",
    "    return classifier, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c14a8",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Example: Train Linear SVM on two moons dataset with different noise levels\n",
    "\n",
    "# Example 1: Low noise - SVM struggles with non-linear boundary\n",
    "print(\"Example 1: Linear SVM on low noise two moons\")\n",
    "X1, y1, mfig1 = generate_two_moons(n_samples=200, noise=0.05, show_plot=False)\n",
    "plt.close(mfig1)\n",
    "clf1, fig1 = classify_with_svm(X1, y1, show_plot=True)\n",
    "print(f\"Training accuracy: {clf1.score(X1, y1):.3f}\")\n",
    "print(\"Note: Linear boundary cannot perfectly separate the moons\")\n",
    "print()\n",
    "\n",
    "# Example 2: Medium noise\n",
    "print(\"Example 2: Linear SVM on medium noise two moons\")\n",
    "X2, y2, mfig2 = generate_two_moons(n_samples=200, noise=0.15, show_plot=False)\n",
    "plt.close(mfig2)\n",
    "clf2, fig2 = classify_with_svm(X2, y2, show_plot=True)\n",
    "print(f\"Training accuracy: {clf2.score(X2, y2):.3f}\")\n",
    "print()\n",
    "\n",
    "# Example 3: High noise\n",
    "print(\"Example 3: Linear SVM on high noise two moons\")\n",
    "X3, y3, mfig3 = generate_two_moons(n_samples=200, noise=0.25, show_plot=False)\n",
    "plt.close(mfig3)\n",
    "clf3, fig3 = classify_with_svm(X3, y3, show_plot=True)\n",
    "print(f\"Training accuracy: {clf3.score(X3, y3):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20fac3c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb2965b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## **Part C**: Neural Network Classification\n",
    "\n",
    "Implement `classify_with_mlp(X, y, show_plot=False)` that trains a **Multi-Layer Perceptron (MLP)** classifier and visualizes the decision boundary.\n",
    "\n",
    "### Background: Multi-Layer Perceptron\n",
    "\n",
    "A **Multi-Layer Perceptron (MLP)** is a type of artificial neural network with multiple layers of neurons. Unlike linear SVMs, MLPs can learn **non-linear decision boundaries** by using:\n",
    "\n",
    "1. **Hidden layers** with non-linear activation functions\n",
    "2. **Multiple neurons** that can combine features in complex ways\n",
    "3. **Backpropagation** to adjust weights and learn from data\n",
    "\n",
    "**Architecture**:\n",
    "- **Input layer**: Receives the features (2 in our case)\n",
    "- **Hidden layer(s)**: Transforms features with non-linear activations\n",
    "- **Output layer**: Produces class predictions\n",
    "\n",
    "**Key Advantage**: MLPs can learn complex, non-linear decision boundaries that adapt to the data structure, making them ideal for datasets like the two moons.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "This part is very similar to Part B, but uses an MLP instead of a linear SVM.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Train the classifier**:\n",
    "   - Use `sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=rng_seed)`\n",
    "     - `hidden_layer_sizes=(100, 50)`: Two hidden layers with 100 and 50 neurons\n",
    "     - `max_iter=1000`: Maximum number of training iterations\n",
    "   - Fit the model using `classifier.fit(X, y)`\n",
    "\n",
    "2. **Create visualization meshgrid** (same as Part B):\n",
    "   - Create meshgrid covering the data range with 0.5 padding and 0.02 step size\n",
    "   - Predict on all meshgrid points\n",
    "   - Reshape predictions to match meshgrid shape\n",
    "\n",
    "3. **Plot configuration**:\n",
    "   - Figure size: (10, 8)\n",
    "   - Use `ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu_r')` to show decision regions\n",
    "   - Scatter plot training data with Class 0 in blue and Class 1 in red\n",
    "   - X-axis label: \"Feature 1\"\n",
    "   - Y-axis label: \"Feature 2\"\n",
    "   - Title: \"Neural Network (MLP) Decision Boundary\"\n",
    "   - Grid with alpha=0.3\n",
    "   - Legend\n",
    "   - Equal aspect ratio using `ax.set_aspect('equal')`\n",
    "   - Only call `plt.show()` if `show_plot=True`\n",
    "\n",
    "**Parameters:**\n",
    "- `X`: numpy array of shape (n_samples, 2), feature matrix from Part A\n",
    "- `y`: numpy array of shape (n_samples,), label vector from Part A\n",
    "- `show_plot`: bool, default False. If True, display the plot\n",
    "\n",
    "**Returns:**\n",
    "- `classifier`: trained MLPClassifier object\n",
    "- `fig`: matplotlib figure object (or None if show_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac871d8d",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def classify_with_mlp(X, y, show_plot=False):\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    \n",
    "    # Train the MLP classifier\n",
    "    # Use: MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=rng_seed)\n",
    "    # Fit using: classifier.fit(X, y)\n",
    "    \n",
    "    #   1. Create meshgrid (same as Part B)\n",
    "    #   2. Predict class for all meshgrid points\n",
    "    #   3. Use contourf to show decision regions\n",
    "    #   4. Scatter plot training data\n",
    "    #   5. Set labels, title (use \"Neural Network (MLP) Decision Boundary\"), grid, legend\n",
    "\n",
    "    if show_plot:    \n",
    "        # Show plot if requested\n",
    "        plt.show()\n",
    "\n",
    "    return classifier, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93586f47",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Example: Train Neural Network on two moons dataset and compare with SVM\n",
    "\n",
    "# Example 1: Low noise - MLP learns non-linear boundary\n",
    "print(\"Example 1: Neural Network on low noise two moons\")\n",
    "X1, y1, mfig = generate_two_moons(n_samples=200, noise=0.05, show_plot=False)\n",
    "plt.close(mfig)\n",
    "mlp1, fig1 = classify_with_mlp(X1, y1, show_plot=True)\n",
    "print(f\"Training accuracy: {mlp1.score(X1, y1):.3f}\")\n",
    "print(\"Note: Non-linear boundary fits the moon shapes much better!\")\n",
    "print()\n",
    "\n",
    "# Example 2: Medium noise\n",
    "print(\"Example 2: Neural Network on medium noise two moons\")\n",
    "X2, y2, mfig = generate_two_moons(n_samples=200, noise=0.15, show_plot=False)\n",
    "plt.close(mfig)\n",
    "mlp2, fig2 = classify_with_mlp(X2, y2, show_plot=True)\n",
    "print(f\"Training accuracy: {mlp2.score(X2, y2):.3f}\")\n",
    "print()\n",
    "\n",
    "# Example 3: High noise\n",
    "print(\"Example 3: Neural Network on high noise two moons\")\n",
    "X3, y3, mfig = generate_two_moons(n_samples=200, noise=0.25, show_plot=False)\n",
    "plt.close(mfig)\n",
    "mlp3, fig3 = classify_with_mlp(X3, y3, show_plot=True)\n",
    "print(f\"Training accuracy: {mlp3.score(X3, y3):.3f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d59db6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0152244b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# **Question 2**: Handwritten Digit Classification with Neural Networks\n",
    "\n",
    "In this question, you'll build a **multi-layer perceptron (MLP)** to classify handwritten digits from the classic **MNIST-style digits dataset**. This is a fundamental problem in machine learning and computer vision.\n",
    "\n",
    "## Background: Handwritten Digit Recognition\n",
    "\n",
    "**Handwritten digit recognition** is a multi-class classification problem where we want to identify which digit (0-9) is shown in an image. This task has practical applications in:\n",
    "- Reading ZIP codes on mail\n",
    "- Processing bank checks\n",
    "- Digitizing handwritten documents\n",
    "- Automatic form processing\n",
    "\n",
    "## The Digits Dataset\n",
    "\n",
    "The digits dataset contains:\n",
    "- **1797 samples** of 8×8 pixel grayscale images of handwritten digits (0-9)\n",
    "- Each image is represented as 64 features (flattened 8×8 pixels)\n",
    "- 10 classes (digits 0 through 9)\n",
    "- Pixel values range from 0 (white) to 16 (black)\n",
    "\n",
    "Unlike the two moons dataset, this is a **multi-class classification** problem with real-world image data.\n",
    "\n",
    "## **Part A**: Load and Split the Dataset\n",
    "\n",
    "Implement `load_digits_dataset(train_fraction, show_samples=False)` that loads the handwritten digits dataset and splits it into training and validation sets.\n",
    "\n",
    "**Requirements:**\n",
    "- Use `sklearn.datasets.load_digits()` to load the dataset\n",
    "- Access features as `X = digits.data` and labels as `y = digits.target`\n",
    "- Use `sklearn.model_selection.train_test_split()` to split the data:\n",
    "  - `train_size=train_fraction`\n",
    "  - `random_state=rng_seed`\n",
    "  - `stratify=y` (ensures balanced class distribution in both sets)\n",
    "- If `show_samples=True`, create a figure showing 10 random samples from the training set:\n",
    "  - Figure size: (12, 4)\n",
    "  - Use `plt.subplot(2, 5, i+1)` for 2 rows and 5 columns\n",
    "  - Display each image using `ax.imshow(image.reshape(8, 8), cmap='gray')`\n",
    "  - Title each subplot with its label: `f'Label: {label}'`\n",
    "  - Turn off axis: `ax.axis('off')`\n",
    "  - Only call `plt.show()` if `show_samples=True`\n",
    "- Return the figure object (or None if `show_samples=False`)\n",
    "\n",
    "**Parameters:**\n",
    "- `train_fraction`: float, fraction of data to use for training (rest is validation)\n",
    "- `show_samples`: bool, default False. If True, display sample images\n",
    "\n",
    "**Returns:**\n",
    "- `X_train`: numpy array, training features\n",
    "- `X_val`: numpy array, validation features\n",
    "- `y_train`: numpy array, training labels\n",
    "- `y_val`: numpy array, validation labels\n",
    "- `fig`: matplotlib figure object (or None if show_samples=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341fbc5f",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def load_digits_dataset(train_fraction, show_samples=False):\n",
    "    from sklearn.datasets import load_digits\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Load the digits dataset\n",
    "    # Access features as digits.data and labels as digits.target\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    # Use train_test_split with train_size=train_fraction, random_state=rng_seed, stratify=y\n",
    "    \n",
    "    # If show_samples=True:\n",
    "    #   - Select 10 random samples from training set\n",
    "    #   - Create 2x5 subplot grid\n",
    "    #   - Display each image (reshaped to 8x8) with gray colormap\n",
    "    #   - Set title to show label\n",
    "    #   - Turn off axes\n",
    "    #   - Call plt.show()\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf44e31",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Example: Load and visualize the digits dataset\n",
    "\n",
    "print(\"Loading digits dataset with 75% training / 25% validation split\")\n",
    "X_train, X_val, y_train, y_val, fig = load_digits_dataset(train_fraction=0.75, show_samples=True)\n",
    "\n",
    "print(f\"\\nDataset shapes:\")\n",
    "print(f\"  Training: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "print(f\"  Validation: X_val={X_val.shape}, y_val={y_val.shape}\")\n",
    "print(f\"\\nNumber of features: {X_train.shape[1]}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train))}\")\n",
    "print(f\"Classes: {np.unique(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bc0c18",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb982fbf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## **Part B**: Train Neural Network and Visualize Learning\n",
    "\n",
    "Implement `train_digit_classifier(X_train, y_train, X_val, y_val, show_plot=False)` that trains an MLP classifier and visualizes the training process.\n",
    "\n",
    "### Background: Training Neural Networks\n",
    "\n",
    "When training a neural network, it's important to monitor:\n",
    "- **Training loss**: How well the model fits the training data (should decrease over time)\n",
    "- **Validation loss**: How well the model generalizes to unseen data (should also decrease)\n",
    "- **Overfitting**: When training loss decreases but validation loss increases, indicating the model memorizes rather than learns\n",
    "\n",
    "### Your Task: Design Your Architecture\n",
    "\n",
    "You'll need to experiment with the `MLPClassifier` parameters to achieve good performance. Here are the key hyperparameters to explore:\n",
    "\n",
    "**Architecture Parameters:**\n",
    "- `hidden_layer_sizes`: Tuple specifying number of neurons in each hidden layer\n",
    "  - Example: `(100,)` = 1 layer with 100 neurons\n",
    "  - Example: `(128, 64)` = 2 layers with 128 and 64 neurons\n",
    "  - Try different sizes and depths!\n",
    "\n",
    "**Training Parameters:**\n",
    "- `max_iter`: Maximum number of training epochs (iterations)\n",
    "  - More iterations = more training time\n",
    "  - Try 100-500 for reasonable training time\n",
    "- `alpha`: L2 regularization parameter (prevents overfitting)\n",
    "  - Smaller = less regularization\n",
    "  - Try values like 0.0001, 0.001, 0.01\n",
    "- `learning_rate_init`: Initial learning rate\n",
    "  - Controls how fast the model learns\n",
    "  - Try values like 0.001, 0.01\n",
    "\n",
    "**Other Important Parameters:**\n",
    "- `activation`: Activation function for hidden layers\n",
    "  - Options: `'relu'`, `'tanh'`, `'logistic'`\n",
    "  - ReLU is often a good default\n",
    "- `solver`: Optimization algorithm\n",
    "  - Options: `'adam'`, `'sgd'`\n",
    "  - Adam is usually recommended\n",
    "- `random_state=rng_seed`: For reproducibility\n",
    "\n",
    "**Goal**: Achieve **>95% validation accuracy** through experimentation!\n",
    "\n",
    "**Requirements:**\n",
    "1. **Train the classifier**:\n",
    "   - Create `MLPClassifier` with your chosen parameters\n",
    "   - Fit using `classifier.fit(X_train, y_train)`\n",
    "\n",
    "2. **Extract training history**:\n",
    "   - After training, `classifier.loss_curve_` contains the training loss at each epoch\n",
    "\n",
    "3. **Plot configuration** (if `show_plot=True`):\n",
    "   - Figure size: (10, 6)\n",
    "   - Plot training loss curve: `ax.plot(epochs, training_losses, label='Training Loss', linewidth=2)`\n",
    "   - X-axis label: \"Epoch\"\n",
    "   - Y-axis label: \"Loss\"\n",
    "   - Title: \"Training Loss\"\n",
    "   - Grid with alpha=0.3\n",
    "   - Legend\n",
    "   - Only call `plt.show()` if `show_plot=True`\n",
    "\n",
    "4. **Print accuracy** (if print_acc = True):\n",
    "   - Print training accuracy: `classifier.score(X_train, y_train)`\n",
    "   - Print validation accuracy: `classifier.score(X_val, y_val)`\n",
    "\n",
    "**Parameters:**\n",
    "- `X_train`: numpy array, training features from Part A\n",
    "- `y_train`: numpy array, training labels from Part A\n",
    "- `X_val`: numpy array, validation features from Part A\n",
    "- `y_val`: numpy array, validation labels from Part A\n",
    "- `show_plot`: bool, default False. If True, display the plot\n",
    "- `print_acc`: whether to print train/validation accuracy\n",
    "\n",
    "**Returns:**\n",
    "- `classifier`: trained MLPClassifier object\n",
    "- `fig`: matplotlib figure object (or None if show_plot=False)\n",
    "\n",
    "**Note**: For simplicity, you can train once and plot just the training loss curve. Computing validation loss at every epoch requires iterative training with `warm_start=True`, which is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc43279a",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def train_digit_classifier(X_train, y_train, X_val, y_val, show_plot=False, print_acc = False):\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    \n",
    "    # Create and train the MLP classifier\n",
    "    # Experiment with parameters to achieve >95% validation accuracy:\n",
    "    \n",
    "    classifier = MLPClassifier(\n",
    "        # Your parameters here\n",
    "        random_state=rng_seed\n",
    "    )\n",
    "    \n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate and print accuracies\n",
    "    \n",
    "    # Create plot if show_plot=True:\n",
    "    #   - Extract training loss from classifier.loss_curve_\n",
    "    #   - Plot loss vs epoch\n",
    "    #   - Add labels, title, grid, legend\n",
    "    #   - fig = None if show_plot-False\n",
    "    \n",
    "    return classifier, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5b0351",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Example: Train the digit classifier\n",
    "\n",
    "print(\"Training neural network classifier on handwritten digits...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load the dataset\n",
    "X_train, X_val, y_train, y_val, _ = load_digits_dataset(train_fraction=0.75, show_samples=False)\n",
    "\n",
    "# Train the classifier\n",
    "classifier, fig = train_digit_classifier(X_train, y_train, X_val, y_val, show_plot=True, print_acc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f210c4f7",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Example: Test the trained model on individual samples\n",
    "\n",
    "print(\"\\nTesting classifier on individual digit samples\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Select 12 random samples from validation set\n",
    "np.random.seed(rng_seed + 1)\n",
    "test_indices = np.random.choice(len(X_val), size=12, replace=False)\n",
    "\n",
    "# Make predictions\n",
    "predictions = classifier.predict(X_val[test_indices])\n",
    "true_labels = y_val[test_indices]\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, idx in enumerate(test_indices):\n",
    "    image = X_val[idx].reshape(8, 8)\n",
    "    pred = predictions[i]\n",
    "    true = true_labels[i]\n",
    "    \n",
    "    axes[i].imshow(image, cmap='gray')\n",
    "    \n",
    "    # Color the title: green if correct, red if wrong\n",
    "    color = 'green' if pred == true else 'red'\n",
    "    axes[i].set_title(f'True: {true}, Pred: {pred}', color=color, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy on these samples\n",
    "correct = np.sum(predictions == true_labels)\n",
    "print(f\"\\nAccuracy on these {len(test_indices)} samples: {correct}/{len(test_indices)} = {correct/len(test_indices)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24467d31",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42327cd5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Required disclosure of use of AI technology\n",
    "\n",
    "Please indicate whether you used AI to complete this homework. If you did, explain how you used it in the python cell below, as a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e0bd22",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# write ai disclosure here:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc33869",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit.\n",
    "\n",
    "Upload the .zip file to Gradescope!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802a9991",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.export(pdf=False, force_save=True, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4c30a7",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otter_grading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X_test, y_test, fig_test = generate_two_moons(n_samples=100, noise=0.1, show_plot=False)\n>>> test_result = isinstance(X_test, np.ndarray) and isinstance(y_test, np.ndarray) and (X_test.shape == (100, 2)) and (y_test.shape == (100,))\n>>> if fig_test is not None:\n...     plt.close(fig_test)\n>>> test_result\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> X_test, y_test, fig_test = generate_two_moons(n_samples=50, noise=0.1, show_plot=False)\n>>> unique_labels = np.unique(y_test)\n>>> test_result = np.array_equal(unique_labels, np.array([0, 1]))\n>>> if fig_test is not None:\n...     plt.close(fig_test)\n>>> test_result\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> X_test, y_test, fig_test = generate_two_moons(n_samples=50, noise=0.1, show_plot=False)\n>>> test_result = isinstance(fig_test, matplotlib.figure.Figure)\n>>> if fig_test is not None:\n...     plt.close(fig_test)\n>>> test_result\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X_test, y_test, moon_fig = generate_two_moons(n_samples=100, noise=0.1, show_plot=False)\n>>> clf_test, fig_test = classify_with_svm(X_test, y_test, show_plot=False)\n>>> from sklearn.svm import LinearSVC\n>>> plt.close(fig_test)\n>>> plt.close(moon_fig)\n>>> test_result = isinstance(clf_test, LinearSVC)\n>>> test_result\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> X_test, y_test, mfig = generate_two_moons(n_samples=100, noise=0.1, show_plot=False)\n>>> plt.close(mfig)\n>>> clf_test, fig_test = classify_with_svm(X_test, y_test, show_plot=False)\n>>> plt.close(fig_test)\n>>> test_result = hasattr(clf_test, 'decision_function') and callable(clf_test.decision_function)\n>>> test_result\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> X_test, y_test, mplot = generate_two_moons(n_samples=100, noise=0.1, show_plot=False)\n>>> plt.close(mplot)\n>>> clf_test, fig_test = classify_with_svm(X_test, y_test, show_plot=False)\n>>> plt.close(fig_test)\n>>> predictions = clf_test.predict(X_test)\n>>> test_result = predictions.shape == y_test.shape and set(predictions).issubset({0, 1})\n>>> test_result\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> X_test, y_test, mfig = generate_two_moons(n_samples=50, noise=0.1, show_plot=False)\n>>> plt.close(mfig)\n>>> clf_test, fig_test = classify_with_svm(X_test, y_test, show_plot=False)\n>>> plt.close(fig_test)\n>>> test_result = isinstance(fig_test, matplotlib.figure.Figure)\n>>> test_result\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X_test, y_test, mfig = generate_two_moons(n_samples=100, noise=0.1, show_plot=False)\n>>> plt.close(mfig)\n>>> clf_test, fig_test = classify_with_mlp(X_test, y_test, show_plot=False)\n>>> plt.close(fig_test)\n>>> from sklearn.neural_network import MLPClassifier\n>>> test_result = isinstance(clf_test, MLPClassifier)\n>>> test_result\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> X_test, y_test, mfig = generate_two_moons(n_samples=100, noise=0.1, show_plot=False)\n>>> plt.close(mfig)\n>>> clf_test, fig_test = classify_with_mlp(X_test, y_test, show_plot=False)\n>>> plt.close(fig_test)\n>>> test_result = hasattr(clf_test, 'predict_proba') and callable(clf_test.predict_proba)\n>>> test_result\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> X_test, y_test, mfig = generate_two_moons(n_samples=100, noise=0.1, show_plot=False)\n>>> plt.close(mfig)\n>>> clf_test, fig_test = classify_with_mlp(X_test, y_test, show_plot=False)\n>>> plt.close(fig_test)\n>>> predictions = clf_test.predict(X_test)\n>>> test_result = predictions.shape == y_test.shape and set(predictions).issubset({0, 1})\n>>> test_result\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> X_test, y_test, mfig = generate_two_moons(n_samples=200, noise=0.1, show_plot=False)\n>>> plt.close(mfig)\n>>> mlp_test, mlpfig = classify_with_mlp(X_test, y_test, show_plot=False)\n>>> plt.close(mlpfig)\n>>> svm_test, svmfig = classify_with_svm(X_test, y_test, show_plot=False)\n>>> plt.close(svmfig)\n>>> mlp_score = mlp_test.score(X_test, y_test)\n>>> svm_score = svm_test.score(X_test, y_test)\n>>> test_result = mlp_score > 0.9\n>>> test_result\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> X_test, y_test, mfig = generate_two_moons(n_samples=50, noise=0.1, show_plot=False)\n>>> plt.close(mfig)\n>>> clf_test, fig_test = classify_with_mlp(X_test, y_test, show_plot=False)\n>>> plt.close(fig_test)\n>>> test_result = isinstance(fig_test, matplotlib.figure.Figure)\n>>> test_result\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X_train_test, X_val_test, y_train_test, y_val_test, fig_test = load_digits_dataset(train_fraction=0.8, show_samples=False)\n>>> test_result = isinstance(X_train_test, np.ndarray) and isinstance(X_val_test, np.ndarray) and isinstance(y_train_test, np.ndarray) and isinstance(y_val_test, np.ndarray)\n>>> if fig_test is not None:\n...     plt.close(fig_test)\n>>> test_result\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> X_train_test, X_val_test, y_train_test, y_val_test, fig_test = load_digits_dataset(train_fraction=0.7, show_samples=False)\n>>> total = len(X_train_test) + len(X_val_test)\n>>> train_frac = len(X_train_test) / total\n>>> test_result = abs(train_frac - 0.7) < 0.05\n>>> if fig_test is not None:\n...     plt.close(fig_test)\n>>> test_result\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> X_train_test, X_val_test, y_train_test, y_val_test, fig_test = load_digits_dataset(train_fraction=0.75, show_samples=False)\n>>> test_result = X_train_test.shape[1] == 64 and X_val_test.shape[1] == 64\n>>> if fig_test is not None:\n...     plt.close(fig_test)\n>>> test_result\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> X_train_test, X_val_test, y_train_test, y_val_test, fig_test = load_digits_dataset(train_fraction=0.75, show_samples=False)\n>>> unique_labels = np.unique(np.concatenate([y_train_test, y_val_test]))\n>>> test_result = len(unique_labels) == 10 and np.array_equal(unique_labels, np.arange(10))\n>>> if fig_test is not None:\n...     plt.close(fig_test)\n>>> test_result\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X_train_test, X_val_test, y_train_test, y_val_test, _ = load_digits_dataset(train_fraction=0.75, show_samples=False)\n>>> clf_test, fig_test = train_digit_classifier(X_train_test, y_train_test, X_val_test, y_val_test, show_plot=False)\n>>> from sklearn.neural_network import MLPClassifier\n>>> test_result = isinstance(clf_test, MLPClassifier)\n>>> if fig_test is not None:\n...     plt.close(fig_test)\n>>> test_result\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> X_train_test, X_val_test, y_train_test, y_val_test, _ = load_digits_dataset(train_fraction=0.75, show_samples=False)\n>>> clf_test, fig_test = train_digit_classifier(X_train_test, y_train_test, X_val_test, y_val_test, show_plot=False)\n>>> predictions = clf_test.predict(X_val_test)\n>>> test_result = predictions.shape == y_val_test.shape and set(predictions).issubset(set(range(10)))\n>>> if fig_test is not None:\n...     plt.close(fig_test)\n>>> test_result\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> X_train_test, X_val_test, y_train_test, y_val_test, _ = load_digits_dataset(train_fraction=0.75, show_samples=False)\n>>> clf_test, fig_test = train_digit_classifier(X_train_test, y_train_test, X_val_test, y_val_test, show_plot=False)\n>>> val_accuracy = clf_test.score(X_val_test, y_val_test)\n>>> test_result = val_accuracy > 0.85\n>>> if fig_test is not None:\n...     plt.close(fig_test)\n>>> test_result\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> X_train_test, X_val_test, y_train_test, y_val_test, _ = load_digits_dataset(train_fraction=0.75, show_samples=False)\n>>> clf_test, fig_test = train_digit_classifier(X_train_test, y_train_test, X_val_test, y_val_test, show_plot=False)\n>>> test_result = hasattr(clf_test, 'loss_curve_') and len(clf_test.loss_curve_) > 0\n>>> if fig_test is not None:\n...     plt.close(fig_test)\n>>> test_result\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
