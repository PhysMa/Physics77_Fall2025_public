{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb65ddf4",
   "metadata": {},
   "source": [
    "# Week 13 Live Demo — Mini‑MNIST with a Small MLP (scikit‑learn)\n",
    "**Goal:** train a small neural network (multi-layer perception, or MLP) to classify handwritten digits using scikit‑learn, with clear explanations and visuals.\n",
    "\n",
    "**Dataset:** scikit‑learn's built‑in 8×8 **digits** dataset — a reduced MNIST that loads fast and trains quickly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e0a1c",
   "metadata": {},
   "source": [
    "## 1) Load and inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29df2ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use scikit-learn's built-in 'digits' dataset.\n",
    "# - images: 8x8 grayscale pictures of digits 0..9\n",
    "# - data (X): flattened 64-length vectors, one per image\n",
    "# - target (y): the ground-truth digit label 0..9\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "X_full = digits.data            # shape = (n_samples, 64)\n",
    "images_full = digits.images     # shape = (n_samples, 8, 8)\n",
    "y_full = digits.target          # shape = (n_samples,)\n",
    "\n",
    "print('Total samples:', X_full.shape[0])\n",
    "print('Input dimension per sample:', X_full.shape[1])\n",
    "print('Image shape:', images_full.shape[1:], '| Unique labels:', sorted(set(y_full)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77069f9c",
   "metadata": {},
   "source": [
    "## 2) Visualize the dataset (quick overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1844dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing raw data helps build intuition.\n",
    "# We draw a 10x10 grid of random samples and put the label as the small title above each image.\n",
    "\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "rng = np.random.default_rng(7)\n",
    "\n",
    "sel = rng.choice(len(X_full), size=100, replace=False)  # pick 100 random indices\n",
    "fig, axes = plt.subplots(10, 10, figsize=(8,8))\n",
    "for ax, idx in zip(axes.flatten(), sel):\n",
    "    ax.imshow(images_full[idx], cmap='gray')\n",
    "    ax.set_title(int(y_full[idx]), fontsize=8)\n",
    "    ax.axis('off')\n",
    "# plt.suptitle('Random samples from the digits dataset (label shown above each image)', y=0.92)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71228f7",
   "metadata": {},
   "source": [
    "## 3) Train/test split and safe feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0ac8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHY SPLIT?  We need an *honest* test set that the model never sees during training.\n",
    "# WHY STRATIFY?  Keeps class proportions similar in train and test.\n",
    "# WHY SCALE?  MLPs train better when each feature has similar scale (mean 0, std 1).\n",
    "#   Important: fit the scaler on *train only*, then apply to both train and test (avoid data leakage).\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(\n",
    "    X_full, y_full, images_full, test_size=0.25, stratify=y_full, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)   # Learn scaling parameters from *train only*, so each pixel is mean 0, std 1 over the training set\n",
    "X_test_scaled  = scaler.transform(X_test)        # Apply the *same* transform to test\n",
    "\n",
    "print('Train set:', X_train_scaled.shape, ' Test set:', X_test_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb955d9c",
   "metadata": {},
   "source": [
    "## 4) Build a small MLP and train it over several epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf34740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will train a *small* MLP classifier with one hidden layer of 32 neurons.\n",
    "# To collect a *testing curve* (accuracy on test vs epochs), we use incremental training via partial_fit.\n",
    "# Steps per epoch:\n",
    "#   1) Shuffle training data\n",
    "#   2) Loop over mini-batches\n",
    "#   3) Call partial_fit on each batch (first batch must pass 'classes=list_of_labels')\n",
    "# After each epoch, we record:\n",
    "#   - current training loss (mlp.loss_)\n",
    "#   - train accuracy (on X_train_scaled)\n",
    "#   - test accuracy  (on X_test_scaled)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classes = np.unique(y_train)  # array([0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(32,),    # small network, fast to train\n",
    "                    activation='relu',           # try 'tanh' if you want smoother weight patterns\n",
    "                    solver='sgd',                # SGD supports partial_fit for incremental learning\n",
    "                    learning_rate_init=0.01,\n",
    "                    alpha=1e-4,                  # L2 regularization (helps generalization)\n",
    "                    max_iter=1,                  # we do 1 iteration per 'fit' call (unused here)\n",
    "                    warm_start=False,            # not needed when using partial_fit\n",
    "                    random_state=0)\n",
    "\n",
    "# Hyperparameters for our simple training loop\n",
    "epochs = 25\n",
    "batch_size = 64\n",
    "\n",
    "train_loss = []\n",
    "train_acc  = []\n",
    "test_acc   = []\n",
    "\n",
    "n = X_train_scaled.shape[0]\n",
    "indices = np.arange(n)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    # Shuffle each epoch\n",
    "    np.random.shuffle(indices)\n",
    "    X_shuf = X_train_scaled[indices]\n",
    "    y_shuf = y_train[indices]\n",
    "\n",
    "    # Mini-batch loop\n",
    "    for start in range(0, n, batch_size):\n",
    "        end = start + batch_size\n",
    "        X_batch = X_shuf[start:end]\n",
    "        y_batch = y_shuf[start:end]\n",
    "        if start == 0 and epoch == 1:\n",
    "            # First call must include 'classes'\n",
    "            mlp.partial_fit(X_batch, y_batch, classes=classes)\n",
    "        else:\n",
    "            mlp.partial_fit(X_batch, y_batch)\n",
    "\n",
    "    # Record metrics after this epoch\n",
    "    # Note: 'loss_' is training loss on the *last batch* seen; it still shows downward trend over epochs.\n",
    "    train_loss.append(mlp.loss_)\n",
    "    train_acc.append(mlp.score(X_train_scaled, y_train))\n",
    "    test_acc.append(mlp.score(X_test_scaled,  y_test))\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}: loss={train_loss[-1]:.4f}  train_acc={train_acc[-1]:.3f}  test_acc={test_acc[-1]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a103ef",
   "metadata": {},
   "source": [
    "## 5) Training and testing curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0ad0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We plot training loss (should decrease) and train/test accuracy (should rise, then level off).\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,3.6))\n",
    "\n",
    "ax[0].plot(train_loss, marker='o', ms=3)\n",
    "ax[0].set_xlabel('epoch'); ax[0].set_ylabel('training loss'); ax[0].set_title('Loss vs epoch'); ax[0].grid(alpha=0.3)\n",
    "\n",
    "ax[1].plot(train_acc, label='train acc')\n",
    "ax[1].plot(test_acc, label='test acc')\n",
    "ax[1].set_xlabel('epoch'); ax[1].set_ylabel('accuracy'); ax[1].set_title('Accuracy vs epoch'); ax[1].grid(alpha=0.3)\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80316d29",
   "metadata": {},
   "source": [
    "## 6) Confusion matrix (which digits are confused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f6b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(10))\n",
    "fig, ax = plt.subplots(figsize=(5.2,4.2)); disp.plot(ax=ax, cmap='Blues', colorbar=True, values_format='d')\n",
    "ax.set_title('Confusion matrix (test set)'); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9767a8e",
   "metadata": {},
   "source": [
    "## 7) Look at misclassified test images (learn from mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4ac8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing mistakes helps diagnose what patterns the model struggles with.\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "wrong_idx = np.where(y_pred != y_test)[0]\n",
    "print('Misclassified samples:', len(wrong_idx))\n",
    "\n",
    "# Show up to 25 misclassified images with true/pred labels\n",
    "show = wrong_idx[:25]\n",
    "if len(show) > 0:\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(6,6))\n",
    "    for ax, i in zip(axes.flatten(), show):\n",
    "        ax.imshow(images_test[i], cmap='gray')\n",
    "        ax.set_title(f'true={y_test[i]}  pred={y_pred[i]}', fontsize=8)\n",
    "        ax.axis('off')\n",
    "    plt.suptitle('Misclassified test digits')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No misclassifications found in this split; try changing random_state for a different split.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
